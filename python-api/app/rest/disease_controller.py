
import logging
from fastapi import APIRouter, HTTPException, Depends
from typing import Annotated

from app.domain.symptom import Symptom
from app.service.symptom_service import SymptomService
from app.service.llm_service import LLMService


LOG = logging.getLogger(__name__)
disease_router = APIRouter(prefix="/diseases")


def get_llm_service() -> LLMService:
    """Dependency to create LLMService instance."""
    return LLMService()


def get_symptom_service(llm_service: Annotated[LLMService, Depends(get_llm_service)]) -> SymptomService:
    """Dependency to create SymptomService instance with LLM service injected."""
    return SymptomService(llm_service=llm_service)


ServiceDependency = Annotated[SymptomService, Depends(get_symptom_service)]


@disease_router.get("/{disease_name}/symptoms", 
                   response_model=list[Symptom],
                   description="Returns symptoms of the given disease generated by LLM")
async def get_symptoms(disease_name: str, service: ServiceDependency) -> list[Symptom]:
    """
    Get symptoms for a specific disease using LLM generation.
    
    Args:
        disease_name: The name of the disease to get symptoms for
        service: The symptom service dependency
    
    Returns:
        DiseaseSymptoms: Structured response containing symptoms and metadata
    
    Raises:
        HTTPException: If disease name is invalid or service is unavailable
    """
    try:
        LOG.info(f"Received request for disease symptoms: {disease_name}")
        
        result = await service.get_disease_symptoms(disease_name)
        
        return result
        
    except ValueError as e:
        LOG.warning(f"Invalid input for disease: {disease_name}, error: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    
    except Exception as e:
        LOG.error(f"Error getting symptoms for disease {disease_name}: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"Failed to generate symptoms for disease '{disease_name}'. Please ensure the LLM service is running."
        )